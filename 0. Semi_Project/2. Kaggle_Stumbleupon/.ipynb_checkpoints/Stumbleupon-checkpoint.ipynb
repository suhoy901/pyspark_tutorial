{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strumbleupon\n",
    "\n",
    "데이터 : https://www.kaggle.com/c/stumbleupon/data\n",
    " - 데이터는 다운받아서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 변수 특징\n",
    "\n",
    "![](dataSchema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:06.227504Z",
     "start_time": "2018-03-27T08:03:02.109Z"
    }
   },
   "outputs": [],
   "source": [
    "val PATH = \"/home/paulkim/workspace/Spark/semi_project\"\n",
    "val rawData = sc.textFile(\"%s/data/stumbleupon/train.tsv\".format(PATH))\n",
    "val records_full = rawData.map(line => line.split(\"\\t\").map(elem => elem.trim))\n",
    "val header = records_full.first\n",
    "val records = records_full.filter(_(0) != header(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:06.858243Z",
     "start_time": "2018-03-27T08:03:02.283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Array(\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\", \"4042\", \"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machin..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T11:31:45.798671Z",
     "start_time": "2018-03-26T11:31:45.541Z"
    }
   },
   "source": [
    "## 1. Pre-processing\n",
    "1. 텍스트 데이터 활용 X\n",
    "2. 데이터 타입을 모두 Double형으로 변경함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:08.618359Z",
     "start_time": "2018-03-27T08:03:02.574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((0.0,[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val data = records.map{ r => \n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size -1).toInt\n",
    "    val features = trimmed.slice(4, r.size -1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:09.084158Z",
     "start_time": "2018-03-27T08:03:02.721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[4] at map at <console>:32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA 따위는 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:10.704611Z",
     "start_time": "2018-03-27T08:03:03.022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distances : 7395\n"
     ]
    }
   ],
   "source": [
    "val numData = data.count\n",
    "println(\"number of distances : \" + numData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:11.623318Z",
     "start_time": "2018-03-27T08:03:03.162Z"
    }
   },
   "outputs": [],
   "source": [
    "// Naive Bayes 모형을 위해 독립변수에 존재하는 음수를 변경\n",
    "val nbdata = records.map{ r => \n",
    "    val trimmed = r.map(fields => fields.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size -1).toInt\n",
    "    val features = trimmed.slice(4, r.size -1).map(d => if (d == \"?\") 0.0 else d.toDouble).map(d => if (d<0) 0.0 else d)\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:12.011019Z",
     "start_time": "2018-03-27T08:03:03.272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((0.0,[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdata.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Model & Evaluation(accuracy)\n",
    "그냥 깡으로 때려넣음\n",
    "\n",
    "- 로지스틱회귀\n",
    "- SVM\n",
    "- 나이브베이즈\n",
    "- 의사결정나무"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 데이터 전처리 없는 학습모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:12.819046Z",
     "start_time": "2018-03-27T08:03:03.711Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n",
    "import org.apache.spark.mllib.classification.SVMWithSGD\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.configuration.Algo\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:20.548552Z",
     "start_time": "2018-03-27T08:03:03.823Z"
    }
   },
   "outputs": [],
   "source": [
    "val numIters = 10\n",
    "val maxTreeDepth = 5\n",
    "val LR_Model = LogisticRegressionWithSGD.train(data, numIters)\n",
    "val SVM_Model = SVMWithSGD.train(data, numIters)\n",
    "val NB_Model = NaiveBayes.train(nbdata)\n",
    "val DT_Model = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:21.308771Z",
     "start_time": "2018-03-27T08:03:04.117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real label : 0.000000\n",
      "predicted label : 1.000000\n"
     ]
    }
   ],
   "source": [
    "val dataPoint = data.first\n",
    "val trueLabel = dataPoint.label\n",
    "\n",
    "val prediction = LR_Model.predict(dataPoint.features)\n",
    "println(\"real label : %f\".format(trueLabel))\n",
    "println(\"predicted label : %f\".format(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:21.581546Z",
     "start_time": "2018-03-27T08:03:04.262Z"
    }
   },
   "outputs": [],
   "source": [
    "val predictions = LR_Model.predict(data.map(lp => lp.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:22.178078Z",
     "start_time": "2018-03-27T08:03:04.409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionWithSGD Model's Accuacy : 0.514672\n"
     ]
    }
   ],
   "source": [
    "val LR_TotalCorrect = data.map{ point =>\n",
    "    if (LR_Model.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val LR_Accuracy = LR_TotalCorrect / numData\n",
    "println(\"LogisticRegressionWithSGD Model's Accuacy : %f\".format(LR_Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 SVMWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:23.083985Z",
     "start_time": "2018-03-27T08:03:04.708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMWithSGD Model's Accuracy : 0.514672\n"
     ]
    }
   ],
   "source": [
    "val SVM_TotalCorrect = data.map { point =>\n",
    "    if(SVM_Model.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val SVM_Accuracy = SVM_TotalCorrect / numData\n",
    "println(\"SVMWithSGD Model's Accuracy : %f\".format(SVM_Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:24.738579Z",
     "start_time": "2018-03-27T08:03:04.965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBeyes Model's Accuracy : 0.580392\n"
     ]
    }
   ],
   "source": [
    "val NB_TotalCorrect = nbdata.map { point => \n",
    "    if (NB_Model.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val NB_Accuracy = NB_TotalCorrect / numData\n",
    "println(\"NaiveBeyes Model's Accuracy : %f\".format(NB_Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 DecionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:25.683751Z",
     "start_time": "2018-03-27T08:03:05.258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree Model's Accuracy : 0.648276\n"
     ]
    }
   ],
   "source": [
    "val DT_TotalCorrect = data.map { point =>\n",
    "    if (DT_Model.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val DT_Accuracy = DT_TotalCorrect / numData\n",
    "println(\"DecisionTree Model's Accuracy : %f\".format(DT_Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 evaluation\n",
    "**Precision & Recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:29.571736Z",
     "start_time": "2018-03-27T08:03:05.556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
      "SVMModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
      "NaiveBayesModel, Area under PR: 68.0851%, Area under ROC: 58.3559%\n",
      "DecisionTreeModel, Area under PR: 74.3081%, Area under ROC: 64.8837%\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "val metrics = Seq(LR_Model, SVM_Model).map { model =>\n",
    "    val scoreAndLabels = data.map { point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "val nbMetrics = Seq(NB_Model).map{ model =>\n",
    "    val scoreAndLabels = nbdata.map{ point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "val dtMetrics = Seq(DT_Model).map { model =>\n",
    "    val scoreAndLabels = data.map { point => \n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "val allMetrics = metrics ++ nbMetrics ++ dtMetrics\n",
    "allMetrics.foreach{ case (m, pr, roc) =>\n",
    "    println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 데이터 전처리 수행 및 모형 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 기술통계량 확인\n",
    "**mllib.linalg.distributed.RowMatrix** : 열벡터를 RDD형태로 관리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:30.121070Z",
     "start_time": "2018-03-27T08:03:05.993Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "val vectors = data.map(lp => lp.features)\n",
    "val matrix = new RowMatrix(vectors)\n",
    "val matrixSummary = matrix.computeColumnSummaryStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:30.730601Z",
     "start_time": "2018-03-27T08:03:06.147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4122580529952672,2.761823191986608,0.4682304732861389,0.21407992638350232,0.09206236071899916,0.04926216043908053,2.255103452212041,-0.10375042752143335,0.0,0.05642274498417851,0.02123056118999324,0.23377817665490194,0.2757090373659236,0.615551048005409,0.6603110209601082,30.07707910750513,0.03975659229208925,5716.598242055447,178.75456389452353,4.960649087221096,0.17286405047031742,0.10122079189276552]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.045564223,-1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0]\n",
      "[0.999426,363.0,1.0,1.0,0.980392157,0.980392157,21.0,0.25,0.0,0.444444444,1.0,0.716883117,113.3333333,1.0,1.0,100.0,1.0,207952.0,4997.0,22.0,1.0,1.0]\n",
      "[0.10974244167559001,74.30082476809639,0.04126316989120241,0.02153343633200108,0.009211817450882448,0.005274933469767946,32.53918714591821,0.09396988697611545,0.0,0.0017177410346628928,0.020782634824610638,0.0027548394224293036,3.683788919674426,0.2366799607085986,0.22433071201674218,415.8785589543846,0.03818116876739597,7.877330081138463E7,32208.116247426184,10.45300904576431,0.03359363403832393,0.006277532884214705]\n"
     ]
    }
   ],
   "source": [
    "// 독립변수들의 기술통계량을 확인\n",
    "println(matrixSummary.mean)\n",
    "println(matrixSummary.min)\n",
    "println(matrixSummary.max)\n",
    "println(matrixSummary.variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 연속형 데이터 정규화(Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:30.969290Z",
     "start_time": "2018-03-27T08:03:06.472Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:32.014746Z",
     "start_time": "2018-03-27T08:03:06.627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n",
      "[1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,-0.2788207823137022]\n"
     ]
    }
   ],
   "source": [
    "val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\n",
    "val scaledData = data.map(lp => LabeledPoint(lp.label, scaler.transform(lp.features)))\n",
    "\n",
    "println(data.first.features)\n",
    "println(scaledData.first.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 정규화 데이터를  로지스틱에 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:34.190422Z",
     "start_time": "2018-03-27T08:03:06.912Z"
    }
   },
   "outputs": [],
   "source": [
    "val LR_ModelScaled = LogisticRegressionWithSGD.train(scaledData, numIters)\n",
    "val LR_TotalCorrectScaled = scaledData.map { point =>\n",
    "    if(LR_ModelScaled.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val LR_AccuracyScaled = LR_TotalCorrectScaled / numData\n",
    "val LR_PredictVSTrue = scaledData.map { point =>\n",
    "    (LR_ModelScaled.predict(point.features), point.label)\n",
    "}\n",
    "val LR_MetricsScaled = new BinaryClassificationMetrics(LR_PredictVSTrue)\n",
    "val LR_PR = LR_MetricsScaled.areaUnderPR\n",
    "val LR_ROC = LR_MetricsScaled.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:34.451223Z",
     "start_time": "2018-03-27T08:03:07.059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel\n",
      "Accuracy : 62.0419%\n",
      "AreaUnder PR: 72.7254%\n",
      "Area under ROC : 61.9663%\n"
     ]
    }
   ],
   "source": [
    "println(f\"${LR_ModelScaled.getClass.getSimpleName}\\nAccuracy : ${LR_AccuracyScaled * 100}%2.4f%%\\nAreaUnder PR: ${LR_PR * 100.0}%2.4f%%\\nArea under ROC : ${LR_ROC * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 범주형 원핫인코딩 및 데이터 정규화\n",
    "범주형 데이터 원핫인코딩으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:35.282944Z",
     "start_time": "2018-03-27T08:03:07.358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "val categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\n",
    "val numCategories = categories.size\n",
    "println(numCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:35.654622Z",
     "start_time": "2018-03-27T08:03:07.532Z"
    }
   },
   "outputs": [],
   "source": [
    "val dataCategories = records.map { r =>\n",
    "    val trimmed = r.map(fields => fields.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val categoryIdx = categories(r(3))\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    val otherFeatures = trimmed.slice(4, r.size -1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n",
    "    val features = categoryFeatures ++ otherFeatures\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:35.863000Z",
     "start_time": "2018-03-27T08:03:07.688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])\n"
     ]
    }
   ],
   "source": [
    "println(dataCategories.first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:36.590964Z",
     "start_time": "2018-03-27T08:03:07.835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n",
      "[-0.02326210589837061,2.7207366564548514,-0.4464212047941535,-0.22052688457880879,-0.028494000387023734,-0.2709990696925828,-0.23272797709480803,-0.2016540523193296,-0.09914991930875496,-0.38181322324318134,-0.06487757239262681,-0.6807527904251456,-0.20418221057887365,-0.10189469097220732,1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,-0.2788207823137022]\n"
     ]
    }
   ],
   "source": [
    "val scalerCats = new StandardScaler(withMean=true, withStd=true).fit(dataCategories.map(lp => lp.features))\n",
    "val scaledDataCats = dataCategories.map(lp => LabeledPoint(lp.label, scalerCats.transform(lp.features)))\n",
    "println(dataCategories.first.features)\n",
    "println(scaledDataCats.first.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 로지스틱 모형 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:40.747038Z",
     "start_time": "2018-03-27T08:03:08.156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel\n",
      "Accuracy : 66.5720%\n",
      "AreaUnder PR: 75.7964%\n",
      "Area under ROC : 66.5483%\n"
     ]
    }
   ],
   "source": [
    "val LR_ModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIters)\n",
    "val LR_TotalCorrectScaledCats = scaledDataCats.map { point =>\n",
    "    if(LR_ModelScaledCats.predict(point.features) == point.label) 1.0 else 0\n",
    "}.sum\n",
    "val LR_AccuracyScaledCats = LR_TotalCorrectScaledCats / numData\n",
    "val LR_PredictVSTrueCats = scaledDataCats.map { point =>\n",
    "    (LR_ModelScaledCats.predict(point.features), point.label)\n",
    "}\n",
    "val LR_MetricsScaled = new BinaryClassificationMetrics(LR_PredictVSTrueCats)\n",
    "val LR_PRCats = LR_MetricsScaled.areaUnderPR\n",
    "val LR_ROCCats = LR_MetricsScaled.areaUnderROC\n",
    "println(f\"${LR_ModelScaledCats.getClass.getSimpleName}\\nAccuracy : ${LR_AccuracyScaledCats * 100}%2.4f%%\\nAreaUnder PR: ${LR_PRCats * 100.0}%2.4f%%\\nArea under ROC : ${LR_ROCCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 NaiveBayes 학습 및 평가\n",
    "- 범주형 변수만 사용한 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:43.878539Z",
     "start_time": "2018-03-27T08:03:08.453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel\n",
      "Accuracy: 60.9601%\n",
      "Area under PR: 74.0522%\n",
      "Area under ROC: 60.5138%\n"
     ]
    }
   ],
   "source": [
    "val dataNB = records.map{ r =>\n",
    "    val trimmed = r.map(fields => fields.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val categoryIdx = categories(r(3))\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    LabeledPoint(label, Vectors.dense(categoryFeatures))\n",
    "}\n",
    "val NB_ModelCats = NaiveBayes.train(dataNB)\n",
    "val NB_TotalCorrectCats = dataNB.map { point =>\n",
    "    if (NB_ModelCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val NB_AccuracyCats = NB_TotalCorrectCats / numData\n",
    "val NB_PredictVSTrueCats = dataNB.map { point =>\n",
    "    (NB_ModelCats.predict(point.features), point.label)\n",
    "}\n",
    "val NB_MetricsCats = new BinaryClassificationMetrics(NB_PredictVSTrueCats)\n",
    "val NB_PRCats = NB_MetricsCats.areaUnderPR\n",
    "val NB_ROCCats = NB_MetricsCats.areaUnderROC\n",
    "println(f\"${NB_ModelCats.getClass.getSimpleName}\\nAccuracy: ${NB_AccuracyCats * 100}%2.4f%%\\nArea under PR: ${NB_PRCats * 100.0}%2.4f%%\\nArea under ROC: ${NB_ROCCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 모형의 하이퍼파라미터 튜닝\n",
    "### 3.3.1 Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:44.335508Z",
     "start_time": "2018-03-27T08:03:08.747Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.optimization.Updater\n",
    "import org.apache.spark.mllib.optimization.SimpleUpdater\n",
    "import org.apache.spark.mllib.optimization.L1Updater\n",
    "import org.apache.spark.mllib.optimization.SquaredL2Updater\n",
    "import org.apache.spark.mllib.classification.ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:44.687469Z",
     "start_time": "2018-03-27T08:03:08.927Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, \n",
    "updater: Updater, stepSize: Double) = {\n",
    "    val lr = new LogisticRegressionWithSGD\n",
    "    lr.optimizer.setNumIterations(numIterations).setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\n",
    "    lr.run(input)\n",
    "}\n",
    "\n",
    "def createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = {\n",
    "    val scoreAndLabels = data.map { point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (label, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:44.864263Z",
     "start_time": "2018-03-27T08:03:09.072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[236] at map at <console>:48"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledDataCats.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1.1 numIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:49.733642Z",
     "start_time": "2018-03-27T08:03:09.369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations, PR =74.59% , AUC = 64.95%\n",
      "5 iterations, PR =75.80% , AUC = 66.62%\n",
      "10 iterations, PR =75.80% , AUC = 66.55%\n",
      "50 iterations, PR =76.09% , AUC = 66.81%\n",
      "100 iterations, PR =76.08% , AUC = 66.75%\n"
     ]
    }
   ],
   "source": [
    "val iterResults = Seq(1, 5, 10, 50, 100).map{ param =>\n",
    "    val model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)\n",
    "    createMetrics(s\"$param iterations\", scaledDataCats, model)\n",
    "}\n",
    "iterResults.foreach{ case (param, pr, auc) => println(f\"$param, PR =${pr * 100}%2.2f%% , AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1.2 stepSize(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:51.734473Z",
     "start_time": "2018-03-27T08:03:09.662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 step size, PR =74.60%, AUC = 64.97%\n",
      "0.01 step size, PR =74.60%, AUC = 64.96%\n",
      "0.1 step size, PR =75.00%, AUC = 65.52%\n",
      "1.0 step size, PR =75.80%, AUC = 66.55%\n",
      "10.0 step size, PR =72.57%, AUC = 61.92%\n",
      "100.0 step size, PR =64.97%, AUC = 52.06%\n"
     ]
    }
   ],
   "source": [
    "val stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0, 100.0).map { param =>\n",
    "    val model = trainWithParams(scaledDataCats, 0.0, numIters, new SimpleUpdater, param)\n",
    "    createMetrics(s\"$param step size\", scaledDataCats, model)\n",
    "}\n",
    "stepResults.foreach{ case (param, pr, auc) => println(f\"$param, PR =${pr * 100}%2.2f%%, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1.3 regularization\n",
    "- **SimpleUpdater** : regularization을 적용하지 않는 것과 같음. 로지스틱회귀 기본 \n",
    "- **SquaredL2Updater** : 가중치 벡터의 제곱(L2). SVM 모델의 기본\n",
    "- **L1Updater** : 가중치 벡터의 절대값(L1)\n",
    "---\n",
    "참고\n",
    "- Regularization : https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "- L2 norm : https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "- Overfitting vs Underfitting : https://en.wikipedia.org/wiki/Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:53.708713Z",
     "start_time": "2018-03-27T08:03:09.952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 L2 regularization parameter, PR =75.80%, AUC = 66.55%\n",
      "0.01 L2 regularization parameter, PR =75.80%, AUC = 66.55%\n",
      "0.1 L2 regularization parameter, PR =75.84%, AUC = 66.63%\n",
      "1.0 L2 regularization parameter, PR =75.37%, AUC = 66.04%\n",
      "10.0 L2 regularization parameter, PR =52.48%, AUC = 35.33%\n"
     ]
    }
   ],
   "source": [
    "val regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "    val model = trainWithParams(scaledDataCats, param, numIters, new SquaredL2Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\n",
    "}\n",
    "regResults.foreach { case (param, pr, auc) => println(f\"$param, PR =${pr * 100}%2.2f%%, AUC = ${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:55.322509Z",
     "start_time": "2018-03-27T08:03:10.134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 L2 regularization parameter, PR =75.79%, AUC = 66.53%\n",
      "0.01 L2 regularization parameter, PR =75.82%, AUC = 66.47%\n",
      "0.1 L2 regularization parameter, PR =75.67%, AUC = 50.00%\n",
      "1.0 L2 regularization parameter, PR =75.67%, AUC = 50.00%\n",
      "10.0 L2 regularization parameter, PR =75.67%, AUC = 50.00%\n"
     ]
    }
   ],
   "source": [
    "val regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "    val model = trainWithParams(scaledDataCats, param, numIters, new L1Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\n",
    "}\n",
    "regResults.foreach { case (param, pr, auc) => println(f\"$param, PR =${pr * 100}%2.2f%%, AUC = ${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:55.755818Z",
     "start_time": "2018-03-27T08:03:10.428Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.impurity.Impurity\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "import org.apache.spark.mllib.tree.impurity.Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2.1 Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:03:55.934888Z",
     "start_time": "2018-03-27T08:03:10.763Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity:Impurity) = {\n",
    "    DecisionTree.train(input, Algo.Classification, impurity, maxDepth)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:04:05.258594Z",
     "start_time": "2018-03-27T08:03:10.945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 61.68%\n",
      "3 tree depth, AUC = 62.61%\n",
      "4 tree depth, AUC = 63.63%\n",
      "5 tree depth, AUC = 64.88%\n",
      "10 tree depth, AUC = 76.26%\n",
      "20 tree depth, AUC = 98.45%\n",
      "30 tree depth, AUC = 99.95%\n"
     ]
    }
   ],
   "source": [
    "// data : 원 데이터\n",
    "// maxDepth는 30이 최대\n",
    "val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20, 30).map{ param =>\n",
    "    val model = trainDTWithParams(data, param, Entropy)\n",
    "    val scoreAndLabels = data.map{point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "dtResultsEntropy.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:04:14.918690Z",
     "start_time": "2018-03-27T08:03:11.090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 61.68%\n",
      "3 tree depth, AUC = 62.61%\n",
      "4 tree depth, AUC = 63.63%\n",
      "5 tree depth, AUC = 64.88%\n",
      "10 tree depth, AUC = 76.26%\n",
      "20 tree depth, AUC = 98.45%\n",
      "30 tree depth, AUC = 99.95%\n"
     ]
    }
   ],
   "source": [
    "// scaledData : 연속형변수만 정규화된 데이터\n",
    "val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20, 30).map{ param =>\n",
    "    val model = trainDTWithParams(scaledData, param, Entropy)\n",
    "    val scoreAndLabels = scaledData.map{point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "dtResultsEntropy.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:04:24.004789Z",
     "start_time": "2018-03-27T08:03:11.274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 59.33%\n",
      "3 tree depth, AUC = 61.83%\n",
      "4 tree depth, AUC = 62.15%\n",
      "5 tree depth, AUC = 66.50%\n",
      "10 tree depth, AUC = 75.91%\n",
      "20 tree depth, AUC = 96.43%\n",
      "30 tree depth, AUC = 99.74%\n"
     ]
    }
   ],
   "source": [
    "// scaledDataCats : 연속형, 범주형 정규화 데이터\n",
    "val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20, 30).map{ param =>\n",
    "    val model = trainDTWithParams(scaledDataCats, param, Entropy)\n",
    "    val scoreAndLabels = scaledDataCats.map{point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "dtResultsEntropy.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2.2 Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:04:32.550372Z",
     "start_time": "2018-03-27T08:03:11.600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 61.68%\n",
      "3 tree depth, AUC = 62.61%\n",
      "4 tree depth, AUC = 63.63%\n",
      "5 tree depth, AUC = 64.89%\n",
      "10 tree depth, AUC = 78.37%\n",
      "20 tree depth, AUC = 98.87%\n",
      "30 tree depth, AUC = 99.95%\n"
     ]
    }
   ],
   "source": [
    "val dtResultsGini = Seq(1, 2, 3, 4, 5, 10, 20, 30).map{ param =>\n",
    "    val model = trainDTWithParams(data, param, Gini)\n",
    "    val scoreAndLabels = data.map{point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "dtResultsGini.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:04:43.358225Z",
     "start_time": "2018-03-27T08:03:11.785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 61.68%\n",
      "3 tree depth, AUC = 62.61%\n",
      "4 tree depth, AUC = 63.63%\n",
      "5 tree depth, AUC = 64.89%\n",
      "10 tree depth, AUC = 78.37%\n",
      "20 tree depth, AUC = 98.87%\n",
      "30 tree depth, AUC = 99.95%\n"
     ]
    }
   ],
   "source": [
    "val dtResultsGini = Seq(1, 2, 3, 4, 5, 10, 20, 30).map{ param =>\n",
    "    val model = trainDTWithParams(scaledData, param, Gini)\n",
    "    val scoreAndLabels = scaledData.map{point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "dtResultsGini.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:04:55.365489Z",
     "start_time": "2018-03-27T08:03:11.975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 61.61%\n",
      "3 tree depth, AUC = 61.83%\n",
      "4 tree depth, AUC = 62.04%\n",
      "5 tree depth, AUC = 66.45%\n",
      "10 tree depth, AUC = 76.90%\n",
      "20 tree depth, AUC = 98.35%\n",
      "30 tree depth, AUC = 99.93%\n"
     ]
    }
   ],
   "source": [
    "val dtResultsGini = Seq(1, 2, 3, 4, 5, 10, 20, 30).map{ param =>\n",
    "    val model = trainDTWithParams(scaledDataCats, param, Gini)\n",
    "    val scoreAndLabels = scaledDataCats.map{point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "dtResultsGini.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:05:26.754176Z",
     "start_time": "2018-03-27T08:05:26.600Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {\n",
    "    val nb = new NaiveBayes\n",
    "    nb.setLambda(lambda)\n",
    "    nb.run(input)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:05:31.729613Z",
     "start_time": "2018-03-27T08:05:27.908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 lambda, AUC = 60.51%\n",
      "0.01 lambda, AUC = 60.51%\n",
      "0.1 lambda, AUC = 60.51%\n",
      "1.0 lambda, AUC = 60.51%\n",
      "10.0 lambda, AUC = 60.51%\n",
      "100.0 lambda, AUC = 60.51%\n"
     ]
    }
   ],
   "source": [
    "val nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0, 100.0).map { param =>\n",
    "    val model = trainNBWithParams(dataNB, param)\n",
    "    val scoreAndLabels = dataNB.map{ point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param lambda\", metrics.areaUnderROC)\n",
    "}\n",
    "nbResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:05:34.740524Z",
     "start_time": "2018-03-27T08:05:33.131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 L2 regularization parameter, PR = 74.769566%, AUC = 66.126842%\n",
      "0.001 L2 regularization parameter, PR = 74.769566%, AUC = 66.126842%\n",
      "0.0025 L2 regularization parameter, PR = 74.769566%, AUC = 66.126842%\n",
      "0.005 L2 regularization parameter, PR = 74.769566%, AUC = 66.126842%\n",
      "0.01 L2 regularization parameter, PR = 74.741549%, AUC = 66.093195%\n"
     ]
    }
   ],
   "source": [
    "val trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123)\n",
    "val train = trainTestSplit(0)\n",
    "val test = trainTestSplit(1)\n",
    "\n",
    "val regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\n",
    "    val model = trainWithParams(train, param, numIters, new SquaredL2Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", test, model)\n",
    "}\n",
    "regResultsTest.foreach { case (param, pr, auc) => println(f\"$param, PR = ${pr * 100}%2.6f%%, AUC = ${auc * 100}%2.6f%%\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T08:05:36.138039Z",
     "start_time": "2018-03-27T08:05:33.826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 L2 regularization parameter, PR = 76.02586670823597 ,AUC = 66.233459%\n",
      "0.001 L2 regularization parameter, PR = 76.02586670823597 ,AUC = 66.233459%\n",
      "0.0025 L2 regularization parameter, PR = 76.02586670823597 ,AUC = 66.233459%\n",
      "0.005 L2 regularization parameter, PR = 76.04103125345007 ,AUC = 66.257100%\n",
      "0.01 L2 regularization parameter, PR = 76.0584542877353 ,AUC = 66.278745%\n"
     ]
    }
   ],
   "source": [
    "val regResultsTrain = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\n",
    "    val model = trainWithParams(train, param, numIters, new SquaredL2Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", train, model)\n",
    "}\n",
    "regResultsTrain.foreach { case (param, pr, auc) => println(f\"$param, PR = ${pr * 100} ,AUC = ${auc * 100}%2.6f%%\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
