{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe\n",
    "- 데이터프레임은 RDBM의 테이블에서 column이름으로 구성된 immutable한 분산데이터 컬렉션임. Apache Spark 1.3에서 Schema-RDD가 데이터프레임이라는 이름으로 변경됨.\n",
    "- 분산된 데이터 컬렉션에 구조체를 씌움으로써 SparkSQL로 구조적 데이터를 쿼리하거나 lambda대신에 표현함수(expression method)를 사용할 수 있음.\n",
    "- 데이터를 구조적으로 바꿈으로써 스파크 엔진(카탈리스트 옵티마이저)의 스파크 쿼리 성능을 크게 향상시켰음.\n",
    "- RDD에서 쿼리를 파이썬에서 실행하는 것은 자바 JVM과 Py4J 사이의 커뮤티케이션 오버헤드 때문에 크게 느린점을 극복하였음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터프레임 생성하기\n",
    "- SparkSession()을 이용하여 데이터를 import하는 방식으로 데이터프레임을 생성할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON데이터 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:28.605015Z",
     "start_time": "2018-03-02T08:00:28.457131Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringJSONRDD = sc.parallelize([\n",
    "    # id, 이름, 나이, 눈 색깔\n",
    "    \"\"\"{\"id\": \"123\", \"name\": \"Kattie\", \"age\":19, \"eyeColor\" : \"brown\"}\"\"\",\n",
    "    \"\"\"{\"id\": \"234\", \"name\": \"Michael\", \"age\": 22, \"eyeColor\": \"green\"}\"\"\", \n",
    "    \"\"\"{\n",
    "        \"id\": \"345\",\n",
    "        \"name\": \"Simone\",\n",
    "        \"age\": 23,\n",
    "        \"eyeColor\": \"blue\"\n",
    "  }\"\"\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc.parallelize는 아직 실행되지 않는 트랜스포메이션이라는 것을 기억!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:33.152689Z",
     "start_time": "2018-03-02T08:00:30.861779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Kattie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmersJSON = spark.read.json(stringJSONRDD)\n",
    "swimmersJSON.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stringJSONRDD는 spark.read.json을 사용해서 RDD를 데이터프레임으로 변환할 때 실행되는 트랜스포메이션임. 스파크 잡은 spark.read.json을 포함하는 셀을 실행하기 이전까지 실행되지 않는다!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parallelize, map, mapPartitions가 모두 RDD 트랜스포메이션이라는 것은 매우 중요한 점임. 데이터프레임 작업 내에 감싸여서 spark.read.json은 RDD 트랜스포메이션일 뿐만 아니라 RDD를 데이터프레임으로 바꾸는 액션이기도 함(매우 중요함). 데이터프레임 작업을 실행하고 있더라도, 디버깅 목적으로 스파크 UI내에서 RDD작업이 가능함을 알고 있어야 하기 때문임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임시 테이블 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:35.237457Z",
     "start_time": "2018-03-02T08:00:35.132870Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 임시테이블 혹은 테이블을 생성하지 않으면 SparkSQL은 물론이고 SQL도 사용할 수 없음\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임시테이블을 생성하는 것은 데이터프레임 트랜스포메이션이고 데이터프레임 액션이 실행되기 전까지 실행되지 않는다는 것을 잊으면 안됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터프레임 API 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:37.287805Z",
     "start_time": "2018-03-02T08:00:37.063723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123| Kattie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:38.912511Z",
     "start_time": "2018-03-02T08:00:38.320417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Kattie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터를 행 Row()객체로 변환하는 collect()함수를 사용했음. collect()함수나 show()함수는 데이터프레임과 SQL쿼리에 대해 사용할 수 있음. \n",
    " - collect()함수는 데이터프레임의 모든 행을 리턴하고 실행 노드에서 드라이버 노드로 이동하기 때문에 작은 데이터프레임에 사용하는 것이 좋음. \n",
    " - 그렇기 때문에 n을 사용해 리턴되는 행의 갯수를 제한하는 **take**나 **show**를 사용하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:39.956156Z",
     "start_time": "2018-03-02T08:00:39.788929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123| Kattie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from swimmersJSON\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:40.904045Z",
     "start_time": "2018-03-02T08:00:40.759378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Kattie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from swimmersJSON').take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T07:49:48.005602Z",
     "start_time": "2018-03-02T07:49:47.996355Z"
    }
   },
   "source": [
    "databricks notebook을 사용하면 아래와 같은 명령어를 사용할 수 있음\n",
    "버전업? 지원X\n",
    "```python\n",
    "%load_ext sql select * from swimmersJSON\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 프레임과 RDD연동\n",
    "기존에 있는 RDD를 데이터프레임(Dataset[T])으로 변경하는 방법은 2가지\n",
    "- 리플렉션(reflection)을 사용해 스키마를 추측하는 것\n",
    "- 스키마를 직접적으로 코드상에 명싱하는 방법\n",
    "\n",
    "리플렉션을 사용하면 더욱 자세한 코드를 작성할 수 있는 반면에, 스키마를 명시하는 방법은 열과 데이터 타입이 런타임에 드러날 때 데이터프레임의 구조를 만들도록해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. reflection을 이용한 스키마 추측하기\n",
    "데이터프레임을 빌드하고 쿼리를 수행하는 과정에서 이 데이터프레임에 대한 스키마는 자동으로 정의됨. 최초의 행 객체는 key/value 쌍 리스트가 행 클래스의 \\*\\*kwargs로 전달되어 구성됨. 그 후, SparkSQL은 이 행 객체의 RDD를 데이터프레임으로 변경함. 키는 칼럼이고 데이터 타입은 데이터 샘플링을 통해 추측됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:42.462535Z",
     "start_time": "2018-03-02T08:00:42.451191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id를 string이 아니라 long타입으로 변경하는 작업을 수행해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 프로그래밍하는 것처럼 스키마 명시\n",
    "- SparkSQL 데이터 타입을 불러와서 사용\n",
    "- StructField(name, dataType, nullable)\n",
    "    - name : 필드의 이름\n",
    "    - dataType : 필드의 데이터타입\n",
    "    - nullable : 필드가 null이 될 수 있는지 명시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:44.010406Z",
     "start_time": "2018-03-02T08:00:44.001520Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# 콤마로 분리된 데이터를 생성\n",
    "stringCSVRDD = sc.parallelize([\n",
    "    (123, 'Kattie', 19, 'brown'),\n",
    "    (234, 'Michael', 22, 'green'),\n",
    "    (345, 'Simone', 23, 'blue')\n",
    "])\n",
    "\n",
    "# StructType과 StructField를 사용하여 정의\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:45.681486Z",
     "start_time": "2018-03-02T08:00:45.508554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:46.401936Z",
     "start_time": "2018-03-02T08:00:46.398727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터프레임 API로 쿼리하기\n",
    "- 행의 갯수\n",
    "- 필터문 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:50.323587Z",
     "start_time": "2018-03-02T08:00:50.051328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:00:51.787497Z",
     "start_time": "2018-03-02T08:00:51.043786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 방법\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:08.104108Z",
     "start_time": "2018-03-02T08:01:08.100037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, age: bigint, eyeColor: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(swimmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:17.607929Z",
     "start_time": "2018-03-02T08:01:17.376904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "눈 색깔이 b로 시작하는 수영 선수의 이름을 얻어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:18.937851Z",
     "start_time": "2018-03-02T08:01:18.764650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "|Kattie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n",
      "0.16889357566833496\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# 트랜스포메이션 액션을 사용하는 것이 좋음\n",
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()\n",
    "total_time = time.time() - start_time\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL로 쿼리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:20.208244Z",
     "start_time": "2018-03-02T08:01:19.901335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 필터문을 where절을 사용해 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:21.347449Z",
     "start_time": "2018-03-02T08:01:21.186289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "|Kattie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n",
      "0.15664029121398926\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()\n",
    "total_time = time.time() - start_time\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터프레임 시나리오 : 비행 기록 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:22.455847Z",
     "start_time": "2018-03-02T08:01:21.993308Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flightPerFilePath = './flight-data/departuredelays.csv'\n",
    "airportsFilePath = './flight-data/airport-codes-na.txt'\n",
    "\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
    "airports.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:28.985969Z",
     "start_time": "2018-03-02T08:01:28.882960Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 출발지연 데이터셋 획득\n",
    "flightPerf = spark.read.csv(flightPerFilePath, header='true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:29.833353Z",
     "start_time": "2018-03-02T08:01:29.802192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 출발지연 데이터를 캐시\n",
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:30.626736Z",
     "start_time": "2018-03-02T08:01:30.624027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightPerf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:31.333208Z",
     "start_time": "2018-03-02T08:01:31.327677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비행 성능 데이터셋과 공항 데이터셋 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:37.414055Z",
     "start_time": "2018-03-02T08:01:33.541503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select a.City,\n",
    "    f.origin,\n",
    "    sum(f.delay) as Delays\n",
    "    from FlightPerformance f\n",
    "    join airports a\n",
    "    on a.IATA = f.origin\n",
    "    where a.State == 'WA'\n",
    "    group by a.City, f.origin\n",
    "    order by sum(f.delay) desc\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T07:40:35.535957Z",
     "start_time": "2018-03-02T07:40:35.528214Z"
    }
   },
   "source": [
    "databricks를 사용해야 함\n",
    "```python\n",
    "%sql\n",
    "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
    "select a.City, f.origin, sum(f.delay) as Delays\n",
    "from FlightPerformance f\n",
    "join airports a\n",
    "on a.IATA = f.origin\n",
    "where a.State = 'WA'\n",
    "group by a.City, f.origin\n",
    "order by sum(f.delay) desc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T08:01:42.982465Z",
     "start_time": "2018-03-02T08:01:42.402874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select a.City, f.origin, sum(f.delay) as Delays \n",
    "          from FlightPerformance f \n",
    "          join airports a \n",
    "          on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
